{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "\n",
        "    \"\"\"\n",
        "    Splits text into smaller chunks with optional overlaps.\n",
        "    chunk_controller.py\n",
        "    Args:\n",
        "        text (str): The input text to split.\n",
        "        chunk_size (int): The maximum size of each chunk.\n",
        "        overlap (int): The number of overlapping characters between chunks.\n",
        "    Returns:\n",
        "        list: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "_NPdYrDipRQu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "#upload_controller.py\n",
        "UPLOAD_FOLDER = 'data/uploads'\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_file():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "    if file and file.filename.endswith('.pdf'):\n",
        "        filepath = os.path.join(UPLOAD_FOLDER, file.filename)\n",
        "        file.save(filepath)\n",
        "        return jsonify({\"message\": \"File uploaded successfully\", \"path\": filepath}), 200\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Invalid file type. Only PDFs are allowed.\"}), 400\n"
      ],
      "metadata": {
        "id": "klphPlfBFdu0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "#ingestion_controller.py\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the PDF file.\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(filepath)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn5DZ7nlGHS4",
        "outputId": "f4a7f574-f545-44a6-f573-228c5f25eb23"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "#embedding_controller.py\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can replace this with another Sentence Transformers model\n",
        "\n",
        "def generate_embeddings(chunks):\n",
        "    \"\"\"\n",
        "    Generates embeddings for a list of text chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of text chunks.\n",
        "    Returns:\n",
        "        list: A list of embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = model.encode(chunks, convert_to_tensor=True)\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "pvROVmERHvfR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "import faiss\n",
        "import os\n",
        "#vector_controller.py\n",
        "def create_faiss_index(embeddings):\n",
        "    \"\"\"\n",
        "    Creates a FAISS index from embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings (list): List of embeddings to store in the index.\n",
        "    Returns:\n",
        "        faiss.Index: The created FAISS index.\n",
        "    \"\"\"\n",
        "    dim = embeddings[0].shape[0]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings.cpu().numpy())  # Convert PyTorch tensor to NumPy\n",
        "    return index\n",
        "\n",
        "def save_faiss_index(index, path):\n",
        "    \"\"\"\n",
        "    Saves a FAISS index to a file.\n",
        "\n",
        "    Args:\n",
        "        index (faiss.Index): The FAISS index to save.\n",
        "        path (str): File path to save the index.\n",
        "    \"\"\"\n",
        "    faiss.write_index(index, path)\n",
        "\n",
        "def load_faiss_index(path):\n",
        "    \"\"\"\n",
        "    Loads a FAISS index from a file.\n",
        "\n",
        "    Args:\n",
        "        path (str): File path to load the index.\n",
        "    Returns:\n",
        "        faiss.Index: The loaded FAISS index.\n",
        "    \"\"\"\n",
        "    if os.path.exists(path):\n",
        "        return faiss.read_index(path)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Index file not found at {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXo5fv-EISMW",
        "outputId": "f1d3445b-a440-478d-9ad2-b1195b65a440"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Load Pinecone API key from environment variables\n",
        "PINECONE_API_KEY = \"pcsk_622QE6_GnfmM3Pg86QbxSkfX4bMknDwg6pndgxwHEQnVJtqFDwyD2AHngi11MnYPU9LEBk\"\n",
        "# Ensure the required directory exists\n",
        "os.makedirs(\"data/vector_store\", exist_ok=True)\n",
        "\n",
        "# Call the function with your PDF\n",
        "process_pdf_pipeline(\"/content/drive/MyDrive/214M1A3108.pdf\")\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "def upsert_to_pinecone(index_name, embeddings, ids, texts):\n",
        "    \"\"\"\n",
        "    Upserts vectors to a Pinecone index.\n",
        "\n",
        "    Args:\n",
        "        index_name (str): The name of the Pinecone index.\n",
        "        embeddings (list): A list of embeddings.\n",
        "        ids (list): A list of IDs corresponding to the embeddings.\n",
        "        texts (list): A list of metadata text fields for each embedding.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary indicating success or failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the index exists\n",
        "        if index_name not in pc.list_indexes().names():\n",
        "            # Create the index\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=len(embeddings[0]),\n",
        "                metric=\"euclidean\",  # Adjust metric as needed (e.g., cosine)\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-west-2\"  # Adjust the region as needed\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Get the index instance\n",
        "        index = pc.index(index_name)\n",
        "\n",
        "        # Prepare the vectors for upsert\n",
        "        vectors = [{\"id\": str(id_), \"vector\": embedding, \"metadata\": {\"text_field\": text}}\n",
        "                   for id_, embedding, text in zip(ids, embeddings, texts)]\n",
        "\n",
        "        # Upsert the vectors into Pinecone\n",
        "        index.upsert(vectors)\n",
        "\n",
        "        return {\"message\": f\"Upserted {len(embeddings)} vectors to Pinecone index '{index_name}'\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error upserting to Pinecone: {str(e)}\")\n",
        "        return {\"error\": str(e)}\n"
      ],
      "metadata": {
        "id": "-hCUh021Mdmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9f8f15-9262-4258-bf6a-194b3d0673c4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting PDF Processing Pipeline ---\n",
            "\n",
            "[1/4] Extracting text from PDF...\n",
            "Text extraction completed.\n",
            "[2/4] Splitting text into chunks...\n",
            "Text split into 7 chunks.\n",
            "[3/4] Generating embeddings for chunks...\n",
            "Embeddings generation completed.\n",
            "[4/4] Storing embeddings in a FAISS index...\n",
            "FAISS index saved locally.\n",
            "\n",
            "--- PDF Processing Pipeline Completed ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from src.models.controllers.ingestion_controller import extract_text_from_pdf\n",
        "from src.models.controllers.chunk_controller import chunk_text\n",
        "from src.models.controllers.embedding_controller import generate_embeddings\n",
        "from src.models.controllers.vector_controller import create_faiss_index, save_faiss_index\n",
        "from src.models.controllers.pinecone_controller import upsert_to_pinecone\"\"\"\n",
        "\n",
        "def process_pdf_pipeline(filepath, use_pinecone=False):\n",
        "    \"\"\"\n",
        "    Orchestrates the PDF processing pipeline.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the PDF file.\n",
        "        use_pinecone (bool): Whether to store embeddings in Pinecone. Defaults to False.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting PDF Processing Pipeline ---\\n\")\n",
        "\n",
        "    # Step 1: Extract text\n",
        "    print(\"[1/4] Extracting text from PDF...\")\n",
        "    text = extract_text_from_pdf(filepath)\n",
        "    print(\"Text extraction completed.\")\n",
        "\n",
        "    # Step 2: Chunk the text\n",
        "    print(\"[2/4] Splitting text into chunks...\")\n",
        "    chunks = chunk_text(text)\n",
        "    print(f\"Text split into {len(chunks)} chunks.\")\n",
        "\n",
        "    # Step 3: Generate embeddings\n",
        "    print(\"[3/4] Generating embeddings for chunks...\")\n",
        "    embeddings = generate_embeddings(chunks)\n",
        "    print(\"Embeddings generation completed.\")\n",
        "\n",
        "    # Step 4: Store embeddings\n",
        "    if use_pinecone:\n",
        "        print(\"[4/4] Uploading embeddings to Pinecone...\")\n",
        "        upsert_to_pinecone(\"pdf-compliance-index\", embeddings, ids=range(len(chunks)))\n",
        "        print(\"Embeddings uploaded to Pinecone.\")\n",
        "    else:\n",
        "        print(\"[4/4] Storing embeddings in a FAISS index...\")\n",
        "        index = create_faiss_index(embeddings)\n",
        "        save_faiss_index(index, 'data/vector_store/index.faiss')\n",
        "        print(\"FAISS index saved locally.\")\n",
        "\n",
        "    print(\"\\n--- PDF Processing Pipeline Completed ---\\n\")\n"
      ],
      "metadata": {
        "id": "n8xwW-dJKhGe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_pdf_pipeline(\"/content/drive/MyDrive/214M1A3108.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNIltDpBYuxa",
        "outputId": "8020b300-a75d-442e-cc69-5055f223a6e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting PDF Processing Pipeline ---\n",
            "\n",
            "[1/4] Extracting text from PDF...\n",
            "Text extraction completed.\n",
            "[2/4] Splitting text into chunks...\n",
            "Text split into 7 chunks.\n",
            "[3/4] Generating embeddings for chunks...\n",
            "Embeddings generation completed.\n",
            "[4/4] Storing embeddings in a FAISS index...\n",
            "FAISS index saved locally.\n",
            "\n",
            "--- PDF Processing Pipeline Completed ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6g3m_lhaR6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}